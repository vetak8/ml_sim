{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847f8928-cadb-43c4-a21f-f40c4233d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Union, List, Any\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06c7deb-dc7c-4ecd-84a8-3e712f9eaea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        return {}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountTotal(Metric):\n",
    "    \"\"\"Total number of rows in DataFrame\"\"\"\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        return {\"total\": len(df)}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountZeros(Metric):\n",
    "    \"\"\"Number of zeros in choosen column\"\"\"\n",
    "    column: str\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column] == 0)\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "@dataclass\n",
    "class CountNull(Metric):\n",
    "    \"\"\"Number of empty values in choosen columns\"\"\"\n",
    "\n",
    "    columns: List[str]\n",
    "    aggregation: str = \"any\"  # either \"all\", or \"any\"\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        df = df[self.columns].copy()\n",
    "        if self.aggregation == 'all':\n",
    "            k = sum(all(df.loc[i].isna())for i in range(n))\n",
    "        else:\n",
    "            k = sum(any(df.loc[i].isna())for i in range(n))\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "        \n",
    "@dataclass\n",
    "class CountDuplicates(Metric):\n",
    "    \"\"\"Number of duplicates in choosen columns\"\"\"\n",
    "\n",
    "    columns: List[str]\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = len(df[df.duplicated(subset=self.columns)])\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "        \n",
    "@dataclass\n",
    "class CountValue(Metric):\n",
    "    \"\"\"Number of values in choosen column\"\"\"\n",
    "\n",
    "    column: str\n",
    "    value: Union[str, int, float]\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = (df[self.column] == self.value).sum()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edd3f17-fdc5-475a-8c35-12623f5ac018",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CountBelowValue(Metric):\n",
    "    \"\"\"Number of values below threshold\"\"\"\n",
    "\n",
    "    column: str\n",
    "    value: float\n",
    "    strict: bool = False\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        if self.strict:\n",
    "            count_below = (df[self.column] < self.value).sum()\n",
    "        else:\n",
    "            count_below = (df[self.column] <= self.value).sum()\n",
    "        return {\"total\": n, \"count\": count_below, \"delta\": count_below / n}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13e75bc-600f-4901-8132-71a4416929ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('ke_daily_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9e4bf5-216e-47e9-99b6-4adf469de1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = pd.read_csv('ke_visits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f15f978-3151-42e6-9487-aae91c73e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_t = CountTotal()\n",
    "c_z = CountZeros('qty')\n",
    "c_n = CountNull(['price', 'revenue'], 'any')\n",
    "c_d = CountDuplicates(['item_id'])\n",
    "c_v = CountValue('item_id', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b23bab8d-3bd5-459e-8c1a-4120a6752c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 7, 'count': 1, 'delta': 0.14285714285714285}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_z(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b8c5f14-79d5-4aca-9023-c6f24e339a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 9, 'count': 2, 'delta': 0.2222222222222222}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_n(nan_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b6e2eb-f4db-4b0a-9c1f-012f355f3484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 7, 'count': 4, 'delta': 0.5714285714285714}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_d(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81394cc3-a12f-4ec4-9c26-c497e8fc5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import Metric, CountTotal, CountZeros, CountNull, CountDuplicates, CountBelowValue, CountBelowColumn, CountCB, CountRatioBelow, CountLag\n",
    "from checklist import CHECKLIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ff90374b-a20d-4ae1-9b14-5411e345e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "from metrics import Metric\n",
    "import pandas as pd\n",
    "\n",
    "LimitType = Dict[str, Tuple[float, float]]\n",
    "CheckType = Tuple[str, Metric, LimitType]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Report:\n",
    "    \"\"\"DQ report class.\"\"\"\n",
    "\n",
    "    checklist: List[CheckType]\n",
    "    engine: str = \"pandas\"\n",
    "\n",
    "    \n",
    "    _results = {}\n",
    "\n",
    "    def fit(self, tables: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "        \"\"\"Calculate DQ metrics and build report.\"\"\"\n",
    "        self.report_ = {}\n",
    "        report = self.report_\n",
    "        results = []\n",
    "        # Check if engine supported\n",
    "        if self.engine != \"pandas\":\n",
    "            raise NotImplementedError(\"Only pandas API currently supported!\")\n",
    "            \n",
    "        report['title'] = f\"DQ Report for tables {list(tables.keys())}\"\n",
    "        \n",
    "        for table_name, metric, limits in self.checklist:\n",
    "            key = hash(str(tables[table_name].values.tobytes()))  # Create a hash key based on table data\n",
    "            if key in self._results:\n",
    "                result = self._results[key]\n",
    "            else:\n",
    "                result = {}\n",
    "                result['table_name'] = table_name\n",
    "                result['metric'] = str(metric)\n",
    "                result['limits'] = str(limits)\n",
    "                try:\n",
    "                    values = metric(tables[table_name])\n",
    "                    result['values'] = values\n",
    "                    passed = True\n",
    "                    for key, (lower, upper) in limits.items():\n",
    "                        if key not in values:\n",
    "                            passed = False\n",
    "                            break\n",
    "                        value = values[key]\n",
    "                        if not (lower <= value <= upper):\n",
    "                            passed = False\n",
    "                            break\n",
    "                    if passed:\n",
    "                        result['status'] = '.'\n",
    "                        result['error'] = ''\n",
    "                    else:\n",
    "                        result['status'] = 'F'\n",
    "                        result['error'] = ''\n",
    "                except Exception as e:\n",
    "                    result['status'] = 'E'\n",
    "                    result['error'] = type(e).__name__\n",
    "                \n",
    "                results.append(result)\n",
    "        report['result'] = pd.DataFrame.from_records(results)\n",
    "\n",
    "        total_checks = len(report['result'])\n",
    "        passed_checks = report['result']['status'].eq('.').sum()\n",
    "        failed_checks = report['result']['status'].eq('F').sum()\n",
    "        errors = report['result']['status'].eq('E').sum()\n",
    "\n",
    "        report['passed'] = passed_checks\n",
    "        report['failed'] = failed_checks\n",
    "        report['errors'] = errors\n",
    "        report['total'] = total_checks\n",
    "        report['passed_pct'] = (passed_checks / total_checks) * 100 if total_checks != 0 else 0\n",
    "        report['failed_pct'] = (failed_checks / total_checks) * 100 if total_checks != 0 else 0\n",
    "        report['errors_pct'] = (errors / total_checks) * 100 if total_checks != 0 else 0\n",
    "\n",
    "        return report\n",
    "\n",
    "    def to_str(self) -> None:\n",
    "        \"\"\"Convert report to string format.\"\"\"\n",
    "        report = self.report_\n",
    "\n",
    "        msg = (\n",
    "            \"This Report instance is not fitted yet. \"\n",
    "            \"Call 'fit' before usong this method.\"\n",
    "        )\n",
    "\n",
    "        assert isinstance(report, dict), msg\n",
    "\n",
    "        pd.set_option(\"display.max_rows\", 500)\n",
    "        pd.set_option(\"display.max_columns\", 500)\n",
    "        pd.set_option(\"display.max_colwidth\", 20)\n",
    "        pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "        return (\n",
    "            f\"{report['title']}\\n\\n\"\n",
    "            f\"{report['result']}\\n\\n\"\n",
    "            f\"Passed: {report['passed']} ({report['passed_pct']}%)\\n\"\n",
    "            f\"Failed: {report['failed']} ({report['failed_pct']}%)\\n\"\n",
    "            f\"Errors: {report['errors']} ({report['errors_pct']}%)\\n\"\n",
    "            \"\\n\"\n",
    "            f\"Total: {report['total']}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "20424e50-adb0-409a-a1a4-83cf71c97ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {'sales': sales, 'relevance':relevance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6e41e5a0-7e19-43c7-ac05-8e1ce035ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_report = Report(CHECKLIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "18979b15-ef13-4275-a759-cd2e46663079",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_result = dq_report.fit(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2dec885c-6ffa-4228-b5cc-4465483a8edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQ Report for tables ['sales', 'relevance']\n",
      "\n",
      "   table_name               metric               limits               values status     error\n",
      "0       sales         CountTotal()  {'total': (1, 10...         {'total': 7}      .          \n",
      "1       sales  CountLag(column=...      {'lag': (0, 3)}  {'today': '2024-...      F          \n",
      "2       sales  CountDuplicates(...    {'total': (0, 0)}  {'total': 7, 'co...      F          \n",
      "3       sales  CountNull(column...    {'total': (0, 0)}  {'total': 7, 'co...      F          \n",
      "4       sales  CountRatioBelow(...  {'delta': (0, 0....  {'total': 7, 'co...      F          \n",
      "5       sales  CountCB(column='...                   {}  {'lcb': 49.50000...      .          \n",
      "6       sales  CountZeros(colum...  {'delta': (0, 0.3)}  {'total': 7, 'co...      .          \n",
      "7       sales  CountBelowValue(...  {'delta': (0, 0.3)}  {'total': 7, 'co...      .          \n",
      "8   relevance         CountTotal()  {'total': (1, 10...         {'total': 9}      .          \n",
      "9   relevance  CountLag(column=...      {'lag': (0, 3)}                  NaN      E  KeyError\n",
      "10  relevance  CountZeros(colum...  {'delta': (0, 0.2)}  {'total': 9, 'co...      .          \n",
      "11  relevance  CountZeros(colum...  {'delta': (0, 0.5)}  {'total': 9, 'co...      .          \n",
      "12  relevance  CountNull(column...  {'delta': (0, 0.1)}  {'total': 9, 'co...      .          \n",
      "13  relevance  CountBelowValue(...  {'delta': (0, 0.5)}  {'total': 9, 'co...      .          \n",
      "14  relevance  CountBelowColumn...    {'total': (0, 0)}  {'total': 9, 'co...      F          \n",
      "15  relevance  CountBelowColumn...    {'total': (0, 0)}  {'total': 9, 'co...      F          \n",
      "\n",
      "Passed: 9 (56.25%)\n",
      "Failed: 6 (37.5%)\n",
      "Errors: 1 (6.25%)\n",
      "\n",
      "Total: 16\n"
     ]
    }
   ],
   "source": [
    "print(dq_report.to_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "329396fd-2967-4f35-af93-32ef2d13d63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"DQ Report for tables ['sales', 'relevance']\",\n",
       " 'results':    table_name               metric               limits               values status     error\n",
       " 0       sales         CountTotal()  {'total': (1, 10...         {'total': 7}      .          \n",
       " 1       sales  CountLag(column=...      {'lag': (0, 3)}  {'today': '2024-...      F          \n",
       " 2       sales  CountDuplicates(...    {'total': (0, 0)}  {'total': 7, 'co...      F          \n",
       " 3       sales  CountNull(column...    {'total': (0, 0)}  {'total': 7, 'co...      F          \n",
       " 4       sales  CountRatioBelow(...  {'delta': (0, 0....  {'total': 7, 'co...      F          \n",
       " 5       sales  CountCB(column='...                   {}  {'lcb': 49.50000...      .          \n",
       " 6       sales  CountZeros(colum...  {'delta': (0, 0.3)}  {'total': 7, 'co...      .          \n",
       " 7       sales  CountBelowValue(...  {'delta': (0, 0.3)}  {'total': 7, 'co...      .          \n",
       " 8   relevance         CountTotal()  {'total': (1, 10...         {'total': 9}      .          \n",
       " 9   relevance  CountLag(column=...      {'lag': (0, 3)}                  NaN      E  KeyError\n",
       " 10  relevance  CountZeros(colum...  {'delta': (0, 0.2)}  {'total': 9, 'co...      .          \n",
       " 11  relevance  CountZeros(colum...  {'delta': (0, 0.5)}  {'total': 9, 'co...      .          \n",
       " 12  relevance  CountNull(column...  {'delta': (0, 0.1)}  {'total': 9, 'co...      .          \n",
       " 13  relevance  CountBelowValue(...  {'delta': (0, 0.5)}  {'total': 9, 'co...      .          \n",
       " 14  relevance  CountBelowColumn...    {'total': (0, 0)}  {'total': 9, 'co...      F          \n",
       " 15  relevance  CountBelowColumn...    {'total': (0, 0)}  {'total': 9, 'co...      F          ,\n",
       " 'passed': 9,\n",
       " 'failed': 6,\n",
       " 'errors': 1,\n",
       " 'total': 16,\n",
       " 'passed_pct': 56.25,\n",
       " 'failed_pct': 37.5,\n",
       " 'errors_pct': 6.25}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc83f0-44bf-479d-9ef8-3ebceeacbea2",
   "metadata": {},
   "source": [
    "# Адаптация под PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abed25c3-7f00-4fa9-b710-8771e3b2a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Metrics.\"\"\"\n",
    "\n",
    "from typing import Any, Dict, Union, List\n",
    "from dataclasses import dataclass\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql as ps\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metric:\n",
    "    \"\"\"Base class for Metric\"\"\"\n",
    "\n",
    "    def __call__(self, df: Union[pd.DataFrame, ps.DataFrame]) -> Dict[str, Any]:\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            return self._call_pandas(df)\n",
    "\n",
    "        if isinstance(df, ps.DataFrame):\n",
    "            return self._call_pyspark(df)\n",
    "\n",
    "        msg = (\n",
    "            f\"Not supported type of arg 'df': {type(df)}. \"\n",
    "            \"Supported types: pandas.DataFrame, \"\n",
    "            \"pyspark.sql.dataframe.DataFrame\"\n",
    "        )\n",
    "        raise NotImplementedError(msg)\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountTotal(Metric):\n",
    "    \"\"\"Total number of rows in DataFrame\"\"\"\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        return {\"total\": len(df)}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        return {\"total\": df.count()}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountZeros(Metric):\n",
    "    \"\"\"Number of zeros in choosen column\"\"\"\n",
    "\n",
    "    column: str\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column] == 0)\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col, count\n",
    "\n",
    "        n = df.count()\n",
    "        k = df.filter(col(self.column) == 0).count()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "@dataclass\n",
    "class CountNull(Metric):\n",
    "     \"\"\"Number of empty values in chosen columns\"\"\"\n",
    "\n",
    "    columns: List[str]\n",
    "    aggregation: str = \"any\"\n",
    "\n",
    "    def _call__pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        if self.aggregation == 'all':\n",
    "            k = sum(df[self.columns].isna().all(axis=1))\n",
    "        else:\n",
    "            k = sum(df[self.columns].isna().any(axis=1))\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "        n = df.count()\n",
    "        if self.aggregation == 'all':\n",
    "            k = df.filter(col[self.columns].isNull()).count()\n",
    "        else:\n",
    "            k = df.filter(col[self.columns].isNull()).count()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class CountDuplicates(Metric):\n",
    "    \"\"\"Number of duplicates in choosen columns\"\"\"\n",
    "\n",
    "    columns: List[str]\n",
    "\n",
    "    def _call__pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = len(df[df.duplicated(subset=self.columns)])\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "        n = df.count()\n",
    "        k = df.groupBy(self.columns).count().filter(col('count') > 1).count()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "@dataclass\n",
    "class CountValue(Metric):\n",
    "    \"\"\"Number of values in chosen column\"\"\"\n",
    "\n",
    "    column: str\n",
    "    value: Union[str, int, float]\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = (df[self.column] == self.value).sum()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "        n = df.count()\n",
    "        k = df.filter(col(self.column) == self.value).count()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "@dataclass\n",
    "class CountBelowValue(Metric):\n",
    "    \"\"\"Number of values below threshold\"\"\"\n",
    "\n",
    "    column: str\n",
    "    value: float\n",
    "    strict: bool = False\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        if self.strict:\n",
    "            count_below = (df[self.column] < self.value).sum()\n",
    "        else:\n",
    "            count_below = (df[self.column] <= self.value).sum()\n",
    "        return {\"total\": n, \"count\": count_below, \"delta\": count_below / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "\n",
    "        n = df.count()\n",
    "        if self.strict:\n",
    "            count_below = df.filter(col(column) < self.value).count()\n",
    "        else:\n",
    "            count_below = df.filter(col(column) <= self.value).count()\n",
    "            \n",
    "        return {\"total\": n, \"count\": count_below, \"delta\": count_below / n}\n",
    "\n",
    "@dataclass\n",
    "class CountBelowColumn(Metric):\n",
    "    \"\"\"Count how often column X is below column Y\"\"\"\n",
    "\n",
    "    column_x: str\n",
    "    column_y: str\n",
    "    strict: bool = False\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        if self.strict:\n",
    "            count_below = (df[self.column_x] < df[self.column_y]).sum()\n",
    "        else:\n",
    "            count_below = (df[self.column_x] <= df[self.column_y]).sum()\n",
    "        return {\"total\": n, \"count\": count_below, \"delta\": count_below / n}\n",
    "\n",
    "    def _call_pyspark(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "\n",
    "        n = df.count()\n",
    "        if self.strict:\n",
    "            count_below = df.filter(col(self.column_x) < col(self.column_y)).count()\n",
    "        else:\n",
    "            count_below = df.filter(col(self.column_x) <= col(self.column_y)).count()\n",
    "        return {\"total\": n, \"count\": count_below, \"delta\": count_below / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountCB(Metric):\n",
    "    \"\"\"Calculate lower/upper bounds for N%-confidence interval\"\"\"\n",
    "\n",
    "    column: str\n",
    "    conf: float = 0.95\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        q_lower = (1 - self.conf) / 2\n",
    "        q_upper = 1 - q_lower\n",
    "        lcb = df[self.column].quantile(q_lower)\n",
    "        ucb = df[self.column].quantile(q_upper)\n",
    "        return {\"lcb\": lcb, \"ucb\": ucb}\n",
    "        \n",
    "    def _call_pyspark(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "\n",
    "        n = df.count()\n",
    "        quantiles = df.approxQuantile(self.column, [0.5 - self.conf/2, 0.5 + self.conf/2], 0.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ad3663-7491-4dd6-b837-e5fcc1d475c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 7\n",
      "root\n",
      " |-- day: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- qty: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- revenue: string (nullable = true)\n",
      "\n",
      "+----------+-------+---+-----+-------+\n",
      "|       day|item_id|qty|price|revenue|\n",
      "+----------+-------+---+-----+-------+\n",
      "|2022-10-24|    100|  5|120.0|  500.0|\n",
      "|2022-10-24|    100|  6|120.0|  720.0|\n",
      "|2022-10-24|    200|  2|200.0|  400.0|\n",
      "|2022-10-24|    300| 10| 85.0|  850.0|\n",
      "|2022-10-23|    100|  3|110.0|  330.0|\n",
      "|2022-10-23|    200|  8|200.0| 1600.0|\n",
      "|2022-10-23|    300|  0| 90.0|    0.0|\n",
      "+----------+-------+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the PySpark libraries\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# Create a SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "# Load a CSV file\n",
    "df = sqlContext.read.csv(\"./ke_daily_sales.csv\", header=True)\n",
    "\n",
    "# Count the number of rows in the DataFrame\n",
    "rowCount = df.count()\n",
    "\n",
    "# Print the row count\n",
    "print(\"Number of rows:\", rowCount)\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Show the first 10 rows of the DataFrame\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df470bbe-74c4-4619-899d-49f11e01cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnull, window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56297703-9d8e-469a-8192-94bc7b925cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DQ Report.\"\"\"\n",
    "\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "from metrics import Metric\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql as ps\n",
    "\n",
    "LimitType = Dict[str, Tuple[float, float]]\n",
    "CheckType = Tuple[str, Metric, LimitType]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Report:\n",
    "    \"\"\"DQ report class.\"\"\"\n",
    "\n",
    "    checklist: List[CheckType]\n",
    "    engine: str = \"pandas\"\n",
    "    _results = {}\n",
    "    def fit(self, tables: Dict[str, Union[pd.DataFrame, ps.DataFrame]]) -> Dict:\n",
    "        \"\"\"Calculate DQ metrics and build report.\"\"\"\n",
    "\n",
    "        if self.engine == \"pandas\":\n",
    "            return self._fit_pandas(tables)\n",
    "\n",
    "        if self.engine == \"pyspark\":\n",
    "            return self._fit_pyspark(tables)\n",
    "\n",
    "        raise NotImplementedError(\"Only pandas and pyspark APIs currently supported!\")\n",
    "\n",
    "    def _fit_pandas(self, tables: Dict[str, pd.DataFrame]) -> Dict:\n",
    "        \"\"\"Calculate DQ metrics and build report.  Engine: Pandas\"\"\"\n",
    "        self.report_ = {}\n",
    "        report = self.report_\n",
    "\n",
    "        results = []\n",
    "        report['title'] = f\"DQ Report for tables {sorted(list(tables.keys()))}\"\n",
    "        for table_name, metric, limits in self.checklist:\n",
    "            key = hash(str(tables[table_name].values.tobytes()))\n",
    "            if key in self._results:\n",
    "                result = self._results[key]\n",
    "            else:\n",
    "                result = {}\n",
    "                result['table_name'] = table_name\n",
    "                result['metric'] = str(metric)\n",
    "                result['limits'] = str(limits)\n",
    "                try:\n",
    "                    values = metric(tables[table_name])\n",
    "                    result['values'] = values\n",
    "                    passed = True\n",
    "                    for key, (lower, upper) in limits.items():\n",
    "                        if key not in values:\n",
    "                            passed = False\n",
    "                            break\n",
    "                        value = values[key]\n",
    "                        if not (lower <= value <= upper):\n",
    "                            passed = False\n",
    "                            break\n",
    "                    if passed:\n",
    "                        result['status'] = '.'\n",
    "                        result['error'] = ''\n",
    "                    else:\n",
    "                        result['status'] = 'F'\n",
    "                        result['error'] = ''\n",
    "                except Exception as e:\n",
    "                    result['status'] = 'E'\n",
    "                    result['error'] = type(e).__name__\n",
    "                \n",
    "                results.append(result)\n",
    "\n",
    "        report['result'] = pd.DataFrame.from_records(results)\n",
    "\n",
    "        total_checks = len(report['result'])\n",
    "        passed_checks = report['result']['status'].eq('.').sum()\n",
    "        failed_checks = report['result']['status'].eq('F').sum()\n",
    "        errors = report['result']['status'].eq('E').sum()\n",
    "\n",
    "        report['passed'] = passed_checks\n",
    "        report['failed'] = failed_checks\n",
    "        report['errors'] = errors\n",
    "        report['total'] = total_checks\n",
    "        report['passed_pct'] = (passed_checks / total_checks) * 100 if total_checks != 0 else 0\n",
    "        report['failed_pct'] = (failed_checks / total_checks) * 100 if total_checks != 0 else 0\n",
    "        report['errors_pct'] = (errors / total_checks) * 100 if total_checks != 0 else 0\n",
    "\n",
    "        return report\n",
    "\n",
    "    def _fit_pyspark(self, tables: Dict[str, pd.DataFrame]) -> Dict:\n",
    "        \"\"\"Calculate DQ metrics and build report.  Engine: PySpark\"\"\"\n",
    "        self.report_ = {}\n",
    "        report = self.report_\n",
    "\n",
    "        results = []\n",
    "        report['title'] = f\"DQ Report for tables {sorted(list(tables.keys()))}\"\n",
    "        for table_name, metric, limits in self.checklist:\n",
    "            key = hash(str(tables[table_name].collect()))\n",
    "            if key in self._results:\n",
    "                result = self._results[key]\n",
    "            else:\n",
    "                result = {}\n",
    "                result['table_name'] = table_name\n",
    "                result['metric'] = str(metric)\n",
    "                result['limits'] = str(limits)\n",
    "                try:\n",
    "                    values = metric(tables[table_name])\n",
    "                    result['values'] = values\n",
    "                    passed = True\n",
    "                    for key, (lower, upper) in limits.items():\n",
    "                        if key not in values:\n",
    "                            passed = False\n",
    "                            break\n",
    "                        value = values[key]\n",
    "                        if not (lower <= value <= upper):\n",
    "                            passed = False\n",
    "                            break\n",
    "                    if passed:\n",
    "                        result['status'] = '.'\n",
    "                        result['error'] = ''\n",
    "                    else:\n",
    "                        result['status'] = 'F'\n",
    "                        result['error'] = ''\n",
    "                except Exception as e:\n",
    "                    result['status'] = 'E'\n",
    "                    result['error'] = type(e).__name__\n",
    "                \n",
    "                results.append(result)\n",
    "\n",
    "        report['result'] = pd.DataFrame.from_records(results)\n",
    "\n",
    "        total_checks = len(report['result'])\n",
    "        passed_checks = report['result']['status'].eq('.').sum()\n",
    "        failed_checks = report['result']['status'].eq('F').sum()\n",
    "        errors = report['result']['status'].eq('E').sum()\n",
    "\n",
    "        report['passed'] = passed_checks\n",
    "        report['failed'] = failed_checks\n",
    "        report['errors'] = errors\n",
    "        report['total'] = total_checks\n",
    "        report['passed_pct'] = (passed_checks / total_checks) * 100 if total_checks != 0 else 0\n",
    "        report['failed_pct'] = (failed_checks / total_checks) * 100 if total_checks != 0 else 0\n",
    "        report['errors_pct'] = (errors / total_checks) * 100 if total_checks != 0 else 0\n",
    "\n",
    "        return report\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        \"\"\"Convert report to string format.\"\"\"\n",
    "        report = self.report_\n",
    "\n",
    "        msg = (\n",
    "            \"This Report instance is not fitted yet. \"\n",
    "            \"Call 'fit' before using this method.\"\n",
    "        )\n",
    "\n",
    "        assert isinstance(report, dict), msg\n",
    "\n",
    "        pd.set_option(\"display.max_rows\", 500)\n",
    "        pd.set_option(\"display.max_columns\", 500)\n",
    "        pd.set_option(\"display.max_colwidth\", 20)\n",
    "        pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "        return (\n",
    "            f\"{report['title']}\\n\\n\"\n",
    "            f\"{report['result']}\\n\\n\"\n",
    "            f\"Passed: {report['passed']} ({report['passed_pct']}%)\\n\"\n",
    "            f\"Failed: {report['failed']} ({report['failed_pct']}%)\\n\"\n",
    "            f\"Errors: {report['errors']} ({report['errors_pct']}%)\\n\"\n",
    "            \"\\n\"\n",
    "            f\"Total: {report['total']}\"\n",
    "        )\n",
    "\n",
    "\"\"\"Metrics.\"\"\"\n",
    "\n",
    "from typing import Any, Dict, Union, List\n",
    "from dataclasses import dataclass\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql as ps\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metric:\n",
    "    \"\"\"Base class for Metric\"\"\"\n",
    "\n",
    "    def __call__(self, df: Union[pd.DataFrame, ps.DataFrame]) -> Dict[str, Any]:\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            return self._call_pandas(df)\n",
    "\n",
    "        if isinstance(df, ps.DataFrame):\n",
    "            return self._call_pyspark(df)\n",
    "\n",
    "        msg = (\n",
    "            f\"Not supported type of arg 'df': {type(df)}. \"\n",
    "            \"Supported types: pandas.DataFrame, \"\n",
    "            \"pyspark.sql.dataframe.DataFrame\"\n",
    "        )\n",
    "        raise NotImplementedError(msg)\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountTotal(Metric):\n",
    "    \"\"\"Total number of rows in DataFrame\"\"\"\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        return {\"total\": len(df)}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        return {\"total\": df.count()}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountZeros(Metric):\n",
    "    \"\"\"Number of zeros in choosen column\"\"\"\n",
    "\n",
    "    column: str\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = sum(df[self.column] == 0)\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col, count\n",
    "\n",
    "        n = df.count()\n",
    "        k = df.filter(col(self.column) == 0).count()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "@dataclass\n",
    "class CountNull(Metric):\n",
    "    columns: List[str]\n",
    "    aggregation: str = \"any\"\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        if self.aggregation == 'all':\n",
    "            k = sum(df[self.columns].isna().all(axis=1))\n",
    "        else:\n",
    "            k = sum(df[self.columns].isna().any(axis=1))\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col, isnull\n",
    "        \n",
    "        n = df.count()\n",
    "        \n",
    "        # Проверка наличия всех NULL значений в указанных колонках\n",
    "        if self.aggregation == 'all':\n",
    "            condition = isnull(col(self.columns[0]))\n",
    "            for column in self.columns[1:]:\n",
    "                condition &= isnull(col(column))\n",
    "        else:\n",
    "            # Проверка наличия хотя бы одного NULL значения в указанных колонках\n",
    "            condition = isnull(col(self.columns[0]))\n",
    "            for column in self.columns[1:]:\n",
    "                condition |= isnull(col(column))\n",
    "\n",
    "        count_null = df.filter(condition).count()\n",
    "        \n",
    "        return {\"total\": n, \"count\": count_null, \"delta\": count_null / n}\n",
    "\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class CountDuplicates(Metric):\n",
    "    \"\"\"Number of duplicates in chosen columns\"\"\"\n",
    "\n",
    "    columns: List[str]\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = len(df[df.duplicated(subset=self.columns)])\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import count, col, window\n",
    "\n",
    "        window_spec = window.partitionBy(self.columns).orderBy(col(self.columns[0]))\n",
    "        duplicates_count = df.select(self.columns).withColumn(\"row_num\", row_number().over(window_spec)).groupBy(*self.columns).agg(count(\"*\").alias(\"count\")).filter(col(\"count\") > 1).count()\n",
    "        \n",
    "        n = df.count()\n",
    "        return {\"total\": n, \"count\": duplicates_count, \"delta\": duplicates_count / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountValue(Metric):\n",
    "    \"\"\"Number of values in chosen column\"\"\"\n",
    "\n",
    "    column: str\n",
    "    value: Union[str, int, float]\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = (df[self.column] == self.value).sum()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "        n = df.count()\n",
    "        k = df.filter(col(self.column) == self.value).count()\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "@dataclass\n",
    "class CountBelowValue(Metric):\n",
    "    \"\"\"Number of values below threshold\"\"\"\n",
    "\n",
    "    column: str\n",
    "    value: float\n",
    "    strict: bool = False\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        if self.strict:\n",
    "            count_below = (df[self.column] < self.value).sum()\n",
    "        else:\n",
    "            count_below = (df[self.column] <= self.value).sum()\n",
    "        return {\"total\": n, \"count\": count_below, \"delta\": count_below / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "\n",
    "        n = df.count()\n",
    "        if self.strict:\n",
    "            count_below = df.filter(col(self.column) < self.value).count()\n",
    "        else:\n",
    "            count_below = df.filter(col(self.column) <= self.value).count()\n",
    "            \n",
    "        return {\"total\": n, \"count\": count_below, \"delta\": count_below / n}\n",
    "\n",
    "@dataclass\n",
    "class CountBelowColumn(Metric):\n",
    "    \"\"\"Count how often column X is below column Y\"\"\"\n",
    "\n",
    "    column_x: str\n",
    "    column_y: str\n",
    "    strict: bool = False\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        if self.strict:\n",
    "            count_below = (df[self.column_x] < df[self.column_y]).sum()\n",
    "        else:\n",
    "            count_below = (df[self.column_x] <= df[self.column_y]).sum()\n",
    "        return {\"total\": n, \"count\": count_below, \"delta\": count_below / n}\n",
    "\n",
    "    def _call_pyspark(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "\n",
    "        n = df.count()\n",
    "        if self.strict:\n",
    "            count_below = df.filter(col(self.column_x) < col(self.column_y)).count()\n",
    "        else:\n",
    "            count_below = df.filter(col(self.column_x) <= col(self.column_y)).count()\n",
    "        return {\"total\": n, \"count\": count_below, \"delta\": count_below / n}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountCB(Metric):\n",
    "    \"\"\"Calculate lower/upper bounds for N%-confidence interval\"\"\"\n",
    "\n",
    "    column: str\n",
    "    conf: float = 0.95\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        q_lower = (1 - self.conf) / 2\n",
    "        q_upper = 1 - q_lower\n",
    "        lcb = df[self.column].quantile(q_lower)\n",
    "        ucb = df[self.column].quantile(q_upper)\n",
    "        return {\"lcb\": lcb, \"ucb\": ucb}\n",
    "        \n",
    "    def _call_pyspark(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "\n",
    "        n = df.count()\n",
    "        quantiles = df.approxQuantile(self.column, [0.5 - self.conf/2, 0.5 + self.conf/2], 0.0)\n",
    "        lcb, ucb = quantiles[0], quantiles[1]\n",
    "        return {\"lcb\": lcb, \"ucb\": ucb}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CountRatioBelow(Metric):\n",
    "    \"\"\"Count how often X / Y is below Z\"\"\"\n",
    "\n",
    "    column_x: str\n",
    "    column_y: str\n",
    "    column_z: str\n",
    "    strict: bool = False\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        if self.strict:\n",
    "            count_below = ((df[self.column_x] / df[self.column_y]) < df[self.column_z]).sum()\n",
    "        else:\n",
    "            count_below = ((df[self.column_x] / df[self.column_y]) <= df[self.column_z]).sum()\n",
    "        return {\"total\": n, \"count\": count_below, \"delta\": count_below / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import col\n",
    "\n",
    "        n = df.count()\n",
    "        if self.strict:\n",
    "            count_below = df.filter(col(self.column_x) / col(self.column_y) < col(self.column_z)).count()\n",
    "        else:\n",
    "            count_below = df.filter(col(self.column_x) / col(self.column_y) <= col(self.column_z)).count()\n",
    "        return {\"total\": n, \"count\": count_below, \"delta\": count_below / n}\n",
    "\n",
    "@dataclass\n",
    "class CountLag(Metric):\n",
    "    \"\"\"A lag between latest date and today\"\"\"\n",
    "\n",
    "    column: str\n",
    "    fmt: str = \"%Y-%m-%d\"\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        today = datetime.datetime.now().strftime(self.fmt)\n",
    "        df[self.column] = pd.to_datetime(df[self.column], format=self.fmt)\n",
    "        last_day = df[self.column].max().strftime(self.fmt)\n",
    "        last_day_datetime = df[self.column].max()\n",
    "        lag = (datetime.datetime.strptime(today, self.fmt) - last_day_datetime).days\n",
    "        return {\"today\": today, \"last_day\": last_day, \"lag\": lag}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import max, to_date, col\n",
    "\n",
    "        today = datetime.datetime.now().strftime(self.fmt)\n",
    "\n",
    "        # Преобразование столбца к типу данных DateType()\n",
    "        df = df.withColumn(self.column, to_date(col(self.column), self.fmt))\n",
    "\n",
    "        last_day = df.agg(max(col(self.column))).first()[0].strftime(self.fmt)\n",
    "        last_day_datetime = datetime.datetime.strptime(last_day, self.fmt)\n",
    "\n",
    "        # Исправление имени переменной\n",
    "        today_datetime = datetime.datetime.strptime(today, self.fmt)\n",
    "\n",
    "        lag = (today_datetime - last_day_datetime).days\n",
    "        return {\"today\": today, \"last_day\": last_day, \"lag\": lag}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "06adb312-892c-4f3e-a1ee-bb495528497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CountDuplicates(Metric):\n",
    "    \"\"\"Number of duplicates in chosen columns\"\"\"\n",
    "\n",
    "    columns: List[str]\n",
    "\n",
    "    def _call_pandas(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(df)\n",
    "        k = len(df[df.duplicated(subset=self.columns)])\n",
    "        return {\"total\": n, \"count\": k, \"delta\": k / n}\n",
    "\n",
    "    def _call_pyspark(self, df: ps.DataFrame) -> Dict[str, Any]:\n",
    "        from pyspark.sql.functions import count, col, window\n",
    "        \n",
    "        n = df.count()\n",
    "        duplicates_count = n - df.select(self.columns).count()\n",
    "        return {\"total\": n, \"count\": duplicates_count, \"delta\": duplicates_count / n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5de70573-11f2-4166-afe5-15fd038fb691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "from pyspark.sql.functions import count, col, window, row_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f8e15a99-15c4-444a-b3e1-fede933c845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sqlContext.read.csv('ke_daily_sales.csv', header=True)\n",
    "relevance = sqlContext.read.csv('ke_visits.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "195d0440-8579-4f00-ba7d-12ad6dad424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {'sales': sales, 'relevance': relevance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "899ad436-269e-4ee8-baea-227feb86fc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|item_id|\n",
      "+-------+\n",
      "|    200|\n",
      "|    300|\n",
      "|    100|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevance.select('item_id').dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4cb74cbe-494c-4ffa-9ad5-7d1b343384aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+------+--------+\n",
      "|       day|item_id|views|clicks|payments|\n",
      "+----------+-------+-----+------+--------+\n",
      "|2022-09-24|    100| 1000|   219|      56|\n",
      "|2022-09-24|    200| 1248|   343|       1|\n",
      "|2022-09-24|    300|  993|   102|      71|\n",
      "|2022-09-23|    100| 3244|   730|      18|\n",
      "|2022-09-23|    200|  830|   203|       9|\n",
      "|2022-09-23|    300|    0|     0|       2|\n",
      "|2022-09-22|    100| 2130|   123|      20|\n",
      "|2022-09-22|    200| 5320|   500|      13|\n",
      "|2022-09-22|    300|  777|    68|       2|\n",
      "+----------+-------+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e1133e54-5788-44c0-8890-629f01f16366",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_n = CountNull(['views', 'payments'], 'any')\n",
    "c_d = CountDuplicates(['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "aaba5675-c99a-4c42-94c3-747f2f1e9333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 9, 'count': 0, 'delta': 0.0}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_d(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a9ad604d-2c1e-41c8-9b59-996b6d9bd6a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_WINDOWSPEC] Argument `window` should be a WindowSpec, got function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m relevance\u001b[38;5;241m.\u001b[39mselect([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviews\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow_num\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mrow_number\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\column.py:1392\u001b[0m, in \u001b[0;36mColumn.over\u001b[1;34m(self, window)\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WindowSpec\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(window, WindowSpec):\n\u001b[1;32m-> 1392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   1393\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_WINDOWSPEC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1394\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(window)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   1395\u001b[0m     )\n\u001b[0;32m   1396\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jc\u001b[38;5;241m.\u001b[39mover(window\u001b[38;5;241m.\u001b[39m_jspec)\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [NOT_WINDOWSPEC] Argument `window` should be a WindowSpec, got function."
     ]
    }
   ],
   "source": [
    "relevance.select(['views']).withColumn('row_num', row_number().over(window()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b6678-4238-442e-a2a9-f9145b0cc9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
