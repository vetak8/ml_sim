{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6a2a04-f360-42f8-b016-a1b12d6afe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    \"\"\"Gradient boosting regressor.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.base_pred_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model to the data.\n",
    "\n",
    "        Args:\n",
    "            X: array-like of shape (n_samples, n_features)\n",
    "            y: array-like of shape (n_samples,)\n",
    "\n",
    "        Returns:\n",
    "            GradientBoostingRegressor: The fitted model.\n",
    "        \"\"\"\n",
    "        self.base_pred_ = np.mean(y)\n",
    "        return self\n",
    "        \n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the target of new data.\n",
    "\n",
    "        Args:\n",
    "            X: array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns:\n",
    "            y: array-like of shape (n_samples,)\n",
    "            The predict values.\n",
    "            \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE...\n",
    "        predictions = np.full(X.shape[0], self.base_pred_)\n",
    "\n",
    "        return predictions\n",
    "    def _predict_one_sample(self, sample):\n",
    "        return\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1b09f0-cf13-4694-98ab-0dd345f60d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "X = data.values[:,-1]\n",
    "y = data.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919f6fa-d4c1-4b71-8e22-d07ca17edc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "633605d9-5ad2-4ece-acd4-61cd600b6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a44ddd-eef1-4c02-a69d-69571aa6418f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GradientBoostingRegressor at 0x18ca2271bd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81140a06-176a-41fb-a5ec-fb42151b0a96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791,\n",
       "       13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791, 13.791])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e8e069a-1de4-4e8f-a5a4-0f73e66fe3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def mse(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"Mean squared error loss function and gradient.\"\"\"\n",
    "\n",
    "    loss = np.mean((y_pred - y_true) ** 2)\n",
    "    grad = -2 * (y_pred - y_true)\n",
    "    return loss, grad\n",
    "\n",
    "def mae(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"Mean absolute error loss function and gradient.\"\"\"\n",
    "\n",
    "    loss = np.mean(np.abs((y_pred - y_true)))\n",
    "    grad = np.sign((y_pred - y_true))\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ca933e-4f03-4b42-a594-ce8f4765ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=np.array([1, 2, 3])\n",
    "y_pred=np.array([5, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "83fd25c7-e8bb-47b8-86f8-900104294e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union, Tuple\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    '''\n",
    "    Функция реализует градиентный бустинг\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        min_samples_split=2,\n",
    "        loss=\"mse\",\n",
    "        verbose=False,\n",
    "        subsample_size=0.5,\n",
    "        replace=False\n",
    "    ):\n",
    "        self.trees_ = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.loss = loss\n",
    "        self.verbose = verbose\n",
    "        self.base_pred_ = None  \n",
    "        self.subsample_size = subsample_size \n",
    "        self.replace = replace\n",
    "\n",
    "    def _mse(self,y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, np.ndarray]:\n",
    "        \"\"\"Mean squared error loss function and gradient.\"\"\"\n",
    "        error = y_pred - y_true\n",
    "        loss = np.mean(error ** 2)\n",
    "        grad = error\n",
    "        return loss, grad\n",
    "\n",
    "    def _validate_loss_function(self, loss: Union[str, Callable]) -> Callable:\n",
    "        '''Validate loss function'''\n",
    "        if isinstance(loss, str):\n",
    "            if loss == 'mse':\n",
    "                return self._mse\n",
    "        elif callable(loss):\n",
    "            return loss\n",
    "        else:\n",
    "            raise ValueError('Loss must be a callable function')\n",
    "            \n",
    "    def _subsample(self, X, y): \n",
    "        '''\n",
    "        Making subsamples\n",
    "        '''\n",
    "        X = np.array(X)\n",
    "        size = int(np.round(self.subsample_size * X.shape[0]))\n",
    "        subsample_idxs = np.random.choice(X.shape[0], size=size, replace=self.replace)\n",
    "        sub_X = X[subsample_idxs]\n",
    "        sub_y = y[subsample_idxs]\n",
    "        return sub_X, sub_y\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'GradientBoostingRegressor':\n",
    "        \"\"\"Fit the model to the data.\"\"\"\n",
    "        loss_func = self._validate_loss_function(self.loss)\n",
    "    \n",
    "        self.base_pred_ = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.base_pred_, dtype=np.float64)\n",
    "    \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Step 1: Calculate the negative gradient\n",
    "            __, grad = loss_func(y, y_pred)\n",
    "    \n",
    "            # Step 2: Subsample the data\n",
    "            X_subsampled, grad_subsampled = self._subsample(X, grad)\n",
    "            \n",
    "            # Step 3: Fit a decision tree on the subsampled data\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth,\n",
    "                                          min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X_subsampled, -grad_subsampled)\n",
    "    \n",
    "            # Step 4: Append the tree to the list of trees\n",
    "            self.trees_.append(tree)\n",
    "    \n",
    "            # Step 5: Update the predictions based on the new tree\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "    \n",
    "            if self.verbose:\n",
    "                print(f\"Iteration: {_ + 1}, Loss: {loss}\")\n",
    "    \n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the target of new data.\n",
    "\n",
    "        Args:\n",
    "            X: array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns:\n",
    "            y: array-like of shape (n_samples,)\n",
    "            The predict values.\n",
    "\n",
    "        \"\"\"\n",
    "        predictions = np.full(X.shape[0], self.base_pred_)\n",
    "        for tree in self.trees_:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "151c169c-8163-4c44-a010-c78019571e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber(y_true: np.ndarray, y_pred: np.ndarray, delta: float = 0.8) -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"Huber loss function and gradient.\"\"\"\n",
    "    error = y_pred - y_true\n",
    "    absolute_error = np.abs(error)\n",
    "    loss = np.mean(np.where(absolute_error <= delta, 0.5 * error ** 2, delta * (absolute_error - 0.5 * delta)))\n",
    "    grad = np.where(absolute_error <= delta, error, delta * np.sign(error))\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "18716b66-de7c-4a90-bceb-05c5ad93e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "X = data.values[:,:-1]\n",
    "y = data.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "284fb4c7-04f9-46c5-8c1d-08ffef3206ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GradientBoostingRegressor at 0x18cc4283150>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01, max_depth=5, min_samples_split=4, verbose=False, loss='mse')\n",
    "boost.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "786736ae-e179-403f-ad9d-47c8bb07a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2d575a65-8541-4f83-877c-e95e4be11e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37393013703485234"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y[:700], boost.predict(X[:700]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3e83cfa2-c347-4e9a-b0a3-a9aadc3b099e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35097772199564614"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y[700:], boost.predict(X[700:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c3d72-5e91-403f-be17-24ef6e762550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
