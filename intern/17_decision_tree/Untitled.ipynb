{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca011001-c31a-4530-9b45-572baca41199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mse_wmse import mse, weighted_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a56851dd-09f6-4266-8016-16c18c06f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "120fa383-5848-4a94-a642-aa250de40cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X: np.ndarray, y: np.ndarray, feature: int) -> float:\n",
    "    \"\"\"Find the best split for a node (one feature)\"\"\"\n",
    "    best_threshold = None\n",
    "    best_metric = float('inf')\n",
    "    \n",
    "    for split_value in np.unique(X[:, feature]):\n",
    "        left_indices = X[:, feature] <= split_value\n",
    "        right_indices = X[:, feature] > split_value\n",
    "\n",
    "        left_y, right_y = y[left_indices], y[right_indices]\n",
    "        metric = weighted_mse(left_y, right_y)\n",
    "\n",
    "        if metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_threshold = split_value\n",
    "    return best_threshold, best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "30925e97-c350-414a-9eac-b15cbc440784",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df['delay_days'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2b622123-bd36-45b9-a000-9c7ecf8925f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 406.743974171499)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(X, y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "340297ec-f3e9-4c98-ae1a-b953903a4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def best_split(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n",
    "    \"\"\"Find the best split for a node (one feature)\"\"\"\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_metric = float('inf')\n",
    "\n",
    "    for feature in range(X.shape[1]):\n",
    "        threshold, metric = split(X, y, feature)\n",
    "        if metric < best_metric:\n",
    "            best_metric = metric\n",
    "            best_feature = feature\n",
    "            best_threshold = threshold\n",
    "    return best_feature, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "991bac28-6d34-46d5-8317-21a915e4e585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 44443)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0d549-a170-4247-acc7-dffae4a9dca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e974ad83-17ae-4f50-b09e-b1aee711090e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553679e1-28d1-4850-8274-c8f0223a8cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "184df8b7-6623-4605-b925-dde7d34271fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"Decision tree node.\"\"\"\n",
    "    feature: int = None\n",
    "    threshold: float = None\n",
    "    n_samples: int = None\n",
    "    left: Node = None\n",
    "    right: Node = None\n",
    "    mse: float = None\n",
    "    value: int = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecisionTreeRegressor:\n",
    "    \"\"\"Decision tree regressor.\"\"\"\n",
    "    max_depth: int\n",
    "    min_samples_split: int = 2\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> DecisionTreeRegressor:\n",
    "        \"\"\"Build a decision tree regressor from the training set (X, y).\"\"\"\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._split_node(X, y)\n",
    "        return self\n",
    "\n",
    "    def _mse(self, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute the mse criterion for a given set of target values.\"\"\"\n",
    "        squared_error = np.square(y - np.mean(y))\n",
    "        return np.mean(squared_error)\n",
    "\n",
    "    def _weighted_mse(self, y_left: np.ndarray, y_right: np.ndarray) -> float:\n",
    "        \"\"\"Compute the weighted mse criterion for a two given sets of target values\"\"\"\n",
    "        n = len(y_left) + len(y_right)\n",
    "        mse_left = self._mse(y_left)\n",
    "        mse_right = self._mse(y_right)\n",
    "        return (mse_left * len(y_left)) / n + (mse_right * len(y_right)) / n\n",
    "\n",
    "    def _split(self, X: np.ndarray, y: np.ndarray, feature: int) -> float:\n",
    "        \"\"\"Find the best split for a node (one feature)\"\"\"\n",
    "        best_threshold = None\n",
    "        best_metric = float('inf')\n",
    "\n",
    "        unique_values = np.unique(X[:, feature])\n",
    "        if len(unique_values) == 1:\n",
    "            return None, None  # Handle case where all values are the same\n",
    "\n",
    "        for split_value in unique_values:\n",
    "            left_indices = X[:, feature] <= split_value\n",
    "            right_indices = X[:, feature] > split_value\n",
    "\n",
    "            left_y, right_y = y[left_indices], y[right_indices]\n",
    "            metric = self._weighted_mse(left_y, right_y)\n",
    "\n",
    "            if metric < best_metric:\n",
    "                best_metric = metric\n",
    "                best_threshold = split_value\n",
    "        return best_threshold, best_metric\n",
    "\n",
    "    def _best_split(self, X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n",
    "        \"\"\"Find the best split for a node (one feature)\"\"\"\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_metric = float('inf')\n",
    "        same_values_y = np.all(y == y[0])\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            threshold, metric = self._split(X, y, feature)\n",
    "            if same_values_y or metric is None:\n",
    "                continue\n",
    "            if metric < best_metric:\n",
    "                best_metric = metric\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "\n",
    "    def _split_node(self, X: np.ndarray, y: np.ndarray, depth: int = 0) -> Node:\n",
    "        \"\"\"Split a node and return the resulting left and right child nodes.\"\"\"\n",
    "        if depth == self.max_depth or len(X) < self.min_samples_split:\n",
    "            return Node(\n",
    "                feature=None,\n",
    "                threshold=None,\n",
    "                n_samples=len(X),\n",
    "                value=int(np.round(np.mean(y))),  # Use mean value of y for leaf node\n",
    "                mse=self._mse(y)\n",
    "            )\n",
    "\n",
    "        if len(np.unique(y)) == 1:\n",
    "            # All target values are the same, return leaf node with the mean value of y\n",
    "            return Node(\n",
    "                feature=None,\n",
    "                threshold=None,\n",
    "                n_samples=len(X),\n",
    "                value=int(np.round(np.mean(y))),\n",
    "                mse=self._mse(y)\n",
    "            )\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = ~left_indices\n",
    "        left_node = self._split_node(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_node = self._split_node(X[right_indices], y[right_indices], depth + 1)\n",
    "        return Node(feature=best_feature,\n",
    "                    threshold=best_threshold,\n",
    "                    n_samples=len(X),\n",
    "                    mse=self._mse(y),\n",
    "                    value=int(np.round(np.mean(y))),\n",
    "                    left=left_node,\n",
    "                    right=right_node)\n",
    "        \n",
    "    def as_json(self) -> str:\n",
    "        \"\"\"Convert the decision tree to a JSON-like string.\"\"\"\n",
    "        return self._as_json(self.tree_)\n",
    "\n",
    "    def _as_json(self, node: Node) -> str:\n",
    "        \"\"\"Recursively convert the decision tree node to a JSON-like string.\"\"\"\n",
    "        if node.left is None and node.right is None:\n",
    "            # Leaf node\n",
    "            return f'{{\"value\": {node.value}, \"n_samples\": {node.n_samples}, \"mse\": {node.mse}}}'\n",
    "        else:\n",
    "            # Internal node\n",
    "            left_json = self._as_json(node.left)\n",
    "            right_json = self._as_json(node.right)\n",
    "            return f'{{\"feature\": {node.feature}, \"threshold\": {node.threshold}, \"n_samples\": {node.n_samples}, \"mse\": {node.mse}, \"left\": {left_json}, right\": {right_json}}}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ad63ec1a-352a-47eb-9433-bd792dcf64df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"feature\": 1, \"threshold\": 44443, \"n_samples\": 1000, \"mse\": 408.523319, \"left\": {\"feature\": 8, \"threshold\": 370968, \"n_samples\": 406, \"mse\": 600.1995013225268, \"left\": {\"feature\": 9, \"threshold\": 960, \"n_samples\": 320, \"mse\": 267.88374999999996, \"left\": {\"value\": 10, \"n_samples\": 181, \"mse\": 77.6077653307286}, right\": {\"value\": 32, \"n_samples\": 139, \"mse\": 245.38719527974743}}, right\": {\"feature\": 9, \"threshold\": 720, \"n_samples\": 86, \"mse\": 874.1548134126556, \"left\": {\"value\": 30, \"n_samples\": 35, \"mse\": 277.970612244898}, right\": {\"value\": 71, \"n_samples\": 51, \"mse\": 587.6455209534794}}}, right\": {\"feature\": 8, \"threshold\": 361375, \"n_samples\": 594, \"mse\": 76.67012436372707, \"left\": {\"feature\": 9, \"threshold\": 1380, \"n_samples\": 471, \"mse\": 24.289450552422682, \"left\": {\"value\": 1, \"n_samples\": 347, \"mse\": 5.051914723982427}, right\": {\"value\": 7, \"n_samples\": 124, \"mse\": 56.04806191467222}}, right\": {\"feature\": 1, \"threshold\": 52599, \"n_samples\": 123, \"mse\": 196.30590257122086, \"left\": {\"value\": 29, \"n_samples\": 28, \"mse\": 175.95280612244892}, right\": {\"value\": 8, \"n_samples\": 95, \"mse\": 103.13639889196676}}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeRegressor(max_depth=3)\n",
    "decision_tree.fit(X, y)\n",
    "tree_json = decision_tree.as_json()\n",
    "print(tree_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d8f23-ab38-44c7-899c-3f04a1e62e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"Decision tree node.\"\"\"\n",
    "    feature: int = None\n",
    "    threshold: float = None\n",
    "    n_samples: int = None\n",
    "    left: Node = None\n",
    "    right: Node = None\n",
    "    mse: float = None\n",
    "    value: int = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecisionTreeRegressor:\n",
    "    \"\"\"Decision tree regressor.\"\"\"\n",
    "    max_depth: int\n",
    "    min_samples_split: int = 2\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> DecisionTreeRegressor:\n",
    "        \"\"\"Build a decision tree regressor from the training set (X, y).\"\"\"\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._split_node(X, y)\n",
    "        return self\n",
    "\n",
    "    def _mse(self, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute the mse criterion for a given set of target values.\"\"\"\n",
    "        squared_error = np.square(y - np.mean(y))\n",
    "        return np.mean(squared_error)\n",
    "\n",
    "    def _weighted_mse(self, y_left: np.ndarray, y_right: np.ndarray) -> float:\n",
    "        \"\"\"Compute the weighted mse criterion for a two given sets of target values\"\"\"\n",
    "        n = len(y_left) + len(y_right)\n",
    "        mse_left = self._mse(y_left)\n",
    "        mse_right = self._mse(y_right)\n",
    "        return (mse_left * len(y_left)) / n + (mse_right * len(y_right)) / n\n",
    "\n",
    "    def _split(self, X: np.ndarray, y: np.ndarray, feature: int) -> float:\n",
    "        \"\"\"Find the best split for a node (one feature)\"\"\"\n",
    "        best_threshold = None\n",
    "        best_metric = float('inf')\n",
    "\n",
    "        unique_values = np.unique(X[:, feature])\n",
    "        if len(unique_values) == 1:\n",
    "            return None, None  # Handle case where all values are the same\n",
    "\n",
    "        for split_value in unique_values:\n",
    "            left_indices = X[:, feature] <= split_value\n",
    "            right_indices = X[:, feature] > split_value\n",
    "\n",
    "            left_y, right_y = y[left_indices], y[right_indices]\n",
    "            metric = self._weighted_mse(left_y, right_y)\n",
    "\n",
    "            if metric < best_metric:\n",
    "                best_metric = metric\n",
    "                best_threshold = split_value\n",
    "        return best_threshold, best_metric\n",
    "\n",
    "    def _best_split(self, X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n",
    "        \"\"\"Find the best split for a node (one feature)\"\"\"\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_metric = float('inf')\n",
    "        same_values_y = np.all(y == y[0])\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            threshold, metric = self._split(X, y, feature)\n",
    "            if same_values_y or metric is None:\n",
    "                continue\n",
    "            if metric < best_metric:\n",
    "                best_metric = metric\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "\n",
    "    def _split_node(self, X: np.ndarray, y: np.ndarray, depth: int = 0) -> Node:\n",
    "        \"\"\"Split a node and return the resulting left and right child nodes.\"\"\"\n",
    "        if depth == self.max_depth or len(X) < self.min_samples_split:\n",
    "            return Node(\n",
    "                feature=None,\n",
    "                threshold=None,\n",
    "                n_samples=len(X),\n",
    "                value=int(np.round(np.mean(y))),  # Use mean value of y for leaf node\n",
    "                mse=self._mse(y)\n",
    "            )\n",
    "\n",
    "        if len(np.unique(y)) == 1:\n",
    "            # All target values are the same, return leaf node with the mean value of y\n",
    "            return Node(\n",
    "                feature=None,\n",
    "                threshold=None,\n",
    "                n_samples=len(X),\n",
    "                value=int(np.round(np.mean(y))),\n",
    "                mse=self._mse(y)\n",
    "            )\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = ~left_indices\n",
    "        left_node = self._split_node(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_node = self._split_node(X[right_indices], y[right_indices], depth + 1)\n",
    "        return Node(feature=best_feature,\n",
    "                    threshold=best_threshold,\n",
    "                    n_samples=len(X),\n",
    "                    mse=self._mse(y),\n",
    "                    value=int(np.round(np.mean(y))),\n",
    "                    left=left_node,\n",
    "                    right=right_node)\n",
    "        \n",
    "    def as_json(self) -> str:\n",
    "        \"\"\"Convert the decision tree to a JSON-like string.\"\"\"\n",
    "        return self._as_json(self.tree_)\n",
    "\n",
    "    def _as_json(self, node: Node) -> str:\n",
    "        \"\"\"Recursively convert the decision tree node to a JSON-like string.\"\"\"\n",
    "        if node.left is None and node.right is None:\n",
    "            # Leaf node\n",
    "            return f'{{\"value\": {node.value}, \"n_samples\": {node.n_samples}, \"mse\": {node.mse}}}'\n",
    "        else:\n",
    "            # Internal node\n",
    "            left_json = self._as_json(node.left)\n",
    "            right_json = self._as_json(node.right)\n",
    "            return f'{{\"feature\": {node.feature}, \"threshold\": {node.threshold}, \"n_samples\": {node.n_samples}, \"mse\": {node.mse}, \"left\": {left_json}, right\": {right_json}}}'\n",
    "\n",
    "\n",
    "\n",
    "    def _predict_one_sample(self, node: Node, features: np.ndarray) -> int:\n",
    "        \"\"\"Predict the target value of a single sample.\"\"\"\n",
    "            if node.left is None and node.right is None:\n",
    "                # Leaf node, return the value\n",
    "                return node.value\n",
    "            else:\n",
    "                if features[node.feature] <= node.threshold:\n",
    "                    return self._predict_one_sample(node.left, features)\n",
    "                else:\n",
    "                    return self._predict_one_sample(node.right, features)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict regression target for X.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape (n_samples,)\n",
    "            The predicted values.\n",
    "        \"\"\"\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            predictions[i] = self._predict_one_sample(self.tree_, X[i])\n",
    "        return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
