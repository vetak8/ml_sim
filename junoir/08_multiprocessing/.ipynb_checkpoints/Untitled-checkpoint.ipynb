{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9104bd00-6c49-4f62-9554-dfc3a36ac6ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m punctuation\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "\n",
    "def clear_text(text):\n",
    "    \"\"\"Function to clean a single text.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Преобразование к нижнему регистру до удаления знаков пунктуации\n",
    "    text = text.lower()\n",
    "\n",
    "    # Компиляция регулярных выражений\n",
    "    url_pattern = re.compile(r\"https?://[^,\\s]+,?\")\n",
    "    mention_pattern = re.compile(r\"@[^,\\s]+,?\")\n",
    "\n",
    "    # Удаление URL\n",
    "    text = re.sub(url_pattern, \"\", text)\n",
    "\n",
    "    # Удаление упоминаний\n",
    "    text = re.sub(mention_pattern, \"\", text)\n",
    "\n",
    "    # Удаление знаков пунктуации\n",
    "    text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "\n",
    "    # Удаление лишних пробелов\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "\n",
    "    # Преобразование текста в список токенов\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    # Инициализация множества стоп-слов\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Лемматизация слов и исключение стоп-слов\n",
    "    lemma_text = [\n",
    "        lemmatizer.lemmatize(word) for word in text_tokens\n",
    "        if word not in stop_words\n",
    "    ]\n",
    "\n",
    "    # Соединение лемматизированных слов в строку\n",
    "    cleaned_text = \" \".join(lemma_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def clear_data(source_path: str, target_path: str, n_jobs: int):\n",
    "    \"\"\"Parallel process dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_path : str\n",
    "        Path to load dataframe from\n",
    "\n",
    "    target_path : str\n",
    "        Path to save dataframe to\n",
    "\n",
    "    n_jobs : int\n",
    "        Count of job to process\n",
    "    \"\"\"\n",
    "    data = pd.read_parquet(source_path).dropna().reset_index(drop=True)\n",
    "\n",
    "    cleaned_text_list = Parallel(n_jobs=n_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(clear_data)(text) for text in data[\"text\"]\n",
    "    )\n",
    "\n",
    "    data[\"cleaned_text\"] = cleaned_text_list\n",
    "    data.to_parquet(target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82621243-f9e5-49f7-b9ae-ed1bcbf18726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\pc\\carpov_env\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\pc\\carpov_env\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\pc\\carpov_env\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pc\\carpov_env\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\carpov_env\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\carpov_env\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3640b9-f280-49a9-ae7f-edd85e4c3873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
